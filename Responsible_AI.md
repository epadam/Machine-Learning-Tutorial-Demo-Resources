# Responsible AI

## Interpretability

### For All Data Type

* Perturbation Based
   * [LIME](https://github.com/marcotcr/lime)
  
   * [SHAP](https://github.com/slundberg/shap)

* Gradient Based

   * Saliency Maps
      * [Tutorial](https://www.kaggle.com/ernie55ernie/mnist-with-keras-visualization-and-saliency-map)
      * [Keras-vis](https://raghakot.github.io/keras-vis/)

   * [Integrated Gradients](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)
  
   * [Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam)
  
* Relevance Score Based
    
   * [Layerwise Relevance Propagation](https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea)

   * [DeepLift](https://github.com/kundajelab/deeplift)

### For Tabular Data

* Feature Importance

* Permutation Importance

* [xAI](https://github.com/EthicalML/xai)

* [Manifold](https://github.com/uber/manifold#manifold)

* [What If Tool](https://pair-code.github.io/what-if-tool/)

* [InterpretML](https://github.com/interpretml/interpret)

* [TCAV](https://github.com/tensorflow/tcav)


### For Computer Vision



### For Natural Language Processing

* [LIT](https://github.com/PAIR-code/lit)


### Research Papers

Explaining Explanations: An Overview of Interpretability of Machine Learning [`arXiv`](https://arxiv.org/abs/1806.00069)

TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing [`arXiv`](https://arxiv.org/abs/1807.10875)

## Fairness

## Privacy

## Security

## Other Resources

* [Captum](https://captum.ai/)

* [EthicalML/xai](https://github.com/EthicalML/xai)

* [Trusted-AI/AIX360](https://github.com/Trusted-AI/AIX360)

* [Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)

* [https://github.com/pbiecek/xai_resources](https://github.com/pbiecek/xai_resources)
