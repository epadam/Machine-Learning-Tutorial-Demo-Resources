# Responsible AI

## Model Inspection and Explainability

### For Tabular Data

* [SHAP]()

* XAI - An eXplainability toolbox for machine learning [`Github`](https://github.com/EthicalML/xai)

* [LIME](https://github.com/marcotcr/lime)

* [Manifold](https://github.com/uber/manifold#manifold)

* [What If Tool](https://pair-code.github.io/what-if-tool/)

* InterpretML [`Github`](https://github.com/interpretml/interpret)

* Google TCAV [`Github`](https://github.com/tensorflow/tcav)


### Computer Vision

* Feature Map visualization of CNN
  * https://blog.csdn.net/weixin_36411839/article/details/109097714
  * https://kknews.cc/zh-tw/code/4zlpp9v.html

* Perturbation based 

  * [LIME](https://github.com/marcotcr/lime)
  
  * [Integrated Gradients]() 


* Backpropagation based
  
  * Gradient Based

    * Saliency maps
      * [Tutorial](https://www.kaggle.com/ernie55ernie/mnist-with-keras-visualization-and-saliency-map)
      * [Keras-vis](https://raghakot.github.io/keras-vis/)
  
    * [Grad-CAM]()
  
  * Relevance score based
    
    * [Layerwise Relevance Propagation]()

    * [DeepLift]()


### Natural Language Processing

* [LIT]()

* [LIME](https://github.com/marcotcr/lime)




### Research Papers

Explaining Explanations: An Overview of Interpretability of Machine Learning [`arXiv`](https://arxiv.org/abs/1806.00069)

TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing [`arXiv`](https://arxiv.org/abs/1807.10875)

### Other Resources


* [xAI]()

* [360]()

* [https://github.com/pbiecek/xai_resources](https://github.com/pbiecek/xai_resources)
