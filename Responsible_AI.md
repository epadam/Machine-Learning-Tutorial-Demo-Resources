# Responsible AI

## Interpretability

### For All Data Type

* Perturbation Based
   * LIME [`github`](https://github.com/marcotcr/lime)
  
   * SHAP [`github`](https://github.com/slundberg/shap)

* Gradient Based

   * Saliency Map
      * [Tutorial](https://www.kaggle.com/ernie55ernie/mnist-with-keras-visualization-and-saliency-map)
      * [Keras-vis](https://raghakot.github.io/keras-vis/)

   * Integrated Gradients [`link`](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients)
  
   * Grad-CAM (pytorch version) [`github`](https://github.com/jacobgil/pytorch-grad-cam)
  
* Relevance Score Based
    
   * Layerwise Relevance Propagation [`tutorial`](https://towardsdatascience.com/indepth-layer-wise-relevance-propagation-340f95deb1ea)

   * DeepLift [`github`](https://github.com/kundajelab/deeplift)

### For Tabular Data

* Feature Importance

* Permutation Importance

* xAI [`github`](https://github.com/EthicalML/xai)

* Manifold [`github`](https://github.com/uber/manifold#manifold)

* What If Tool [`github`](https://pair-code.github.io/what-if-tool/)

* InterpretML [`github`](https://github.com/interpretml/interpret)

* TCAV [`github`](https://github.com/tensorflow/tcav)


### For Computer Vision



### For Natural Language Processing

* LIT [`github`](https://github.com/PAIR-code/lit)


### Research Papers

Explaining Explanations: An Overview of Interpretability of Machine Learning [`arXiv`](https://arxiv.org/abs/1806.00069)

TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing [`arXiv`](https://arxiv.org/abs/1807.10875)

## Fairness

* Trusted-AI/AIF360 [`github`](https://github.com/Trusted-AI/AIF360)

## Privacy

## Security

## Other Resources

* Captum [`github`](https://captum.ai/)

* EthicalML/xai [`github`](https://github.com/EthicalML/xai)

* Trusted-AI/AIX360 [`github`](https://github.com/Trusted-AI/AIX360)

* [https://github.com/pbiecek/xai_resources](https://github.com/pbiecek/xai_resources)
