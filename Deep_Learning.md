# Deep Learning

## Neural Network

### Back Propagation

How does back propagation work?

### Loss Function

* Binary Cross Entropy 

[notebook]()

* MSE 

[notebook]()

### Optimization

* Adam

### What is Regularization?

[notebook]()

### What is Batch Normalization?

[notebook]()

## Variation of Neural Networks

### CNN

How to calculate the parameters of CNN

### RNN/LSTM/GRN

How to calculate the parameters of RNN


## Domain Adaptation / Transfer Learning

Transfer learning dominates the machine learning today. CV mostly benefit from supervised learning and transfer the features learned to similar tasks. NLP on the other hands benefit more from self-supervised learning. These pre-trained models show incredible performances in many tasks, such as Bert, GPT-2.



## Semi-Supervised Learning (mix labeled and unlabeled data)

[MixMatch](https://github.com/google-research/mixmatch)

## Weakly Supervised Learning

## Self-Supervised Learning (Unsupervised Learning)

Self-supervised learning means training the models without labeled data. AutoEncoder, GAN are examples in CV, while pre-trained model like Bert is example in NLP.

[UGATIT](https://github.com/taki0112/UGATIT?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)

## How to add new class/data to a trained model

* Incremental Training

* Life Long Learning

* Online Learning

## Model IP Protection

How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN [`arXiv`](https://arxiv.org/abs/1903.01743)


## ML Attack

[Breaking neural networks with adversarial attacks](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)



