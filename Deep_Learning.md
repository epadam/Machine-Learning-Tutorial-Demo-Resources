# Deep Learning

### CNN

### Feature Pyramid Networks

### Fully Convolutional Networks

### Capsule Neural Network

* [Capsule Neural Network](https://github.com/Sarasra/models/tree/master/research/capsules)

### Autoencoder

### Variational Autoencoder

### Generative Adversarial Network (GAN)

### VAE-GAN

### Graph Networks

* Relational inductive biases, deep learning, and graph networks [`arXiv`](https://arxiv.org/abs/1806.01261)

* Graph Convolutional Networks

### RNN

### LSTM

### Temporal Convolutional Network

### Attention

### Transformer



## ML Training

### Loss Function

### Optimization

### Regularization

### Batch Normalization

### Tools

[gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)



Here list most important nueral network architecture 

## CNN

How to calculate the parameters of CNN

# RNN/LSTM/GRN

How to calculate the parameters of RNN


Feature Pyramid Networks

Fully Convolutional Networks

Capsule Neural Network [`Github`](https://github.com/Sarasra/models/tree/master/research/capsules)

Autoencoder

Variational Autoencoder

Generative Adversarial Network (GAN)

VAE-GAN

Graph Networks:

Graph Convolutional Networks:

* Relational inductive biases, deep learning, and graph networks [`arXiv`](https://arxiv.org/abs/1806.01261)

* Graph Convolutional Networks

Temporal Convolutional Network



## Semi-Supervised Learning (mix labeled and unlabeled data)

[MixMatch](https://github.com/google-research/mixmatch)

## Weakly Supervised Learning

## Self-Supervised Learning (Unsupervised Learning)

Self-supervised learning means training the models without labeled data. AutoEncoder, GAN are examples in CV, while pre-trained model like Bert is example in NLP.

[UGATIT](https://github.com/taki0112/UGATIT?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)

## How to add new class/data to a trained model

* Incremental Training

* Life Long Learning

* Online Learning

## Reference
* [Awesome Self-Supervised Learning](https://github.com/jason718/awesome-self-supervised-learning)


## Domain Adaptation / Transfer Learning

Transfer learning dominates the machine learning today. CV mostly benefit from supervised learning and transfer the features learned to similar tasks. NLP on the other hands benefit more from self-supervised learning. These pre-trained models show incredible performances in many tasks, such as Bert, GPT-2.

## Model IP Protection

How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN [`arXiv`](https://arxiv.org/abs/1903.01743)

## Anomaly Detection

* Anomaly Detection Learning Resources [`Github`](https://github.com/yzhao062/anomaly-detection-resources)

* Awesome Anomaly Detection [`Github`](https://github.com/zhuyiche/awesome-anomaly-detection)

* awesome anomaly detection [`Github`](https://github.com/hoya012/awesome-anomaly-detection)

### Autoencoder for Anomaly Detection

### GAN for Anomaly Detection

## ML Attack

[Breaking neural networks with adversarial attacks](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)
