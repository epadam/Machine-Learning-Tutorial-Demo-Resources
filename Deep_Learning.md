# Deep Learning

## Neural Network

<img src="https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png" alt="Neural Network" width="300"/>

## Activation Function




### Loss Function

* Classification
  * Binary Cross Entropy 
    [notebook]()
  * 

* Regression
  * MAE 
  * ![MAE](https://miro.medium.com/max/513/0*RWvFBzRzelUnPXpq.png)
  * MSE
  * [notebook]()


### Back Propagation

How does back propagation work?

### Optimization

* Adam

### What is Regularization?

[notebook]()

### What is Batch Normalization?

[notebook]()




## Domain Adaptation / Transfer Learning

Transfer learning dominates the machine learning today. CV mostly benefit from supervised learning and transfer the features learned to similar tasks. NLP on the other hands benefit more from self-supervised learning. These pre-trained models show incredible performances in many tasks, such as Bert, GPT-2.



## Semi-Supervised Learning (mix labeled and unlabeled data)

[MixMatch](https://github.com/google-research/mixmatch)

## Weakly Supervised Learning

## Self-Supervised Learning (Unsupervised Learning)

Self-supervised learning means training the models without labeled data. AutoEncoder, GAN are examples in CV, while pre-trained model like Bert is example in NLP.

[UGATIT](https://github.com/taki0112/UGATIT?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)


## Model IP Protection

How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN [`arXiv`](https://arxiv.org/abs/1903.01743)


## ML Attack

[Breaking neural networks with adversarial attacks](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)



