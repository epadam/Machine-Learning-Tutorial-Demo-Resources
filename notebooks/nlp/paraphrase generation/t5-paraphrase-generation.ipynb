{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('glue', 'qqp')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T14:53:55.638565Z","iopub.execute_input":"2021-09-17T14:53:55.639454Z","iopub.status.idle":"2021-09-17T14:54:35.100019Z","shell.execute_reply.started":"2021-09-17T14:53:55.639407Z","shell.execute_reply":"2021-09-17T14:54:35.099159Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10ac761f5baf4ce98018c830d9070662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6858a19d3a40a08a9748ea3c86dd72"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/41.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524de273ee6f45b58678021dc372d10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = dataset['train']\nvalidation_data = dataset['validation']\ntest_data = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:06:49.727641Z","iopub.execute_input":"2021-09-17T15:06:49.728483Z","iopub.status.idle":"2021-09-17T15:06:49.732730Z","shell.execute_reply.started":"2021-09-17T15:06:49.728448Z","shell.execute_reply":"2021-09-17T15:06:49.731914Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import time\nimport random\nimport math\nimport spacy\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\nfrom torchtext import data, vocab\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:06:52.692151Z","iopub.execute_input":"2021-09-17T15:06:52.693116Z","iopub.status.idle":"2021-09-17T15:06:59.352018Z","shell.execute_reply.started":"2021-09-17T15:06:52.693074Z","shell.execute_reply":"2021-09-17T15:06:59.351282Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:06:59.353511Z","iopub.execute_input":"2021-09-17T15:06:59.353766Z","iopub.status.idle":"2021-09-17T15:07:03.504029Z","shell.execute_reply.started":"2021-09-17T15:06:59.353709Z","shell.execute_reply":"2021-09-17T15:07:03.503299Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2021-09-17 15:06:59.956169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = train_data.filter(lambda example: example['label']==1)\ntrain_data = train_data.remove_columns( ['idx', 'label'])\ntrain_data = train_data.to_csv('train_data.csv')\ntrain_data = pd.read_csv('train_data.csv')\ntrain_data = train_data[['question1', 'question2']]\ntrain_data= train_data[:20000]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:03.507300Z","iopub.execute_input":"2021-09-17T15:07:03.507509Z","iopub.status.idle":"2021-09-17T15:07:19.200452Z","shell.execute_reply.started":"2021-09-17T15:07:03.507486Z","shell.execute_reply":"2021-09-17T15:07:19.199664Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/364 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87d53277a734a1983869d98b9836e96"}},"metadata":{}}]},{"cell_type":"code","source":"class paraDataset(Dataset):\n    def __init__(self, df, tokenizer, source_len):\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.data = df\n        self.source_len = source_len\n        self.summary_len = source_len\n        self.text = df.question1\n        self.para = df.question2\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        text = str(self.text[item])\n        text = \" \".join(text.split())\n\n        para = str(self.para[item])\n        para = \" \".join(para.split())\n\n        source = self.tokenizer.encode_plus(\n            text,\n            max_length=self.source_len,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt')\n        \n        target = self.tokenizer.encode_plus(\n            para,\n            max_length=self.summary_len,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt')\n\n        return {\n            \"source_ids\": source[\"input_ids\"].flatten(),\n            \"source_mask\": source[\"attention_mask\"].flatten(),\n            \"target_ids\": target[\"input_ids\"].flatten(),\n            \"target_mask\": target[\"attention_mask\"].flatten()\n        }","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:19.202075Z","iopub.execute_input":"2021-09-17T15:07:19.202344Z","iopub.status.idle":"2021-09-17T15:07:19.214529Z","shell.execute_reply.started":"2021-09-17T15:07:19.202319Z","shell.execute_reply":"2021-09-17T15:07:19.213863Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,  T5ForConditionalGeneration\nMODEL_NAME = \"t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\")","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:19.215815Z","iopub.execute_input":"2021-09-17T15:07:19.216468Z","iopub.status.idle":"2021-09-17T15:07:29.752438Z","shell.execute_reply.started":"2021-09-17T15:07:19.216428Z","shell.execute_reply":"2021-09-17T15:07:29.751657Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8bc88189dd46a086f138a935d876d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec5ffd5de9240f59c2ab248320a7e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e4fd8fb09e432eb942470e39db4169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ef886f47054047bb5a07c46415a011"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = paraDataset(train_data, tokenizer, 512)\ntrain_data_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\nsample = next(iter(train_data_loader))\nsample['source_ids'].shape, sample['source_mask'].shape, sample['target_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:29.754406Z","iopub.execute_input":"2021-09-17T15:07:29.754684Z","iopub.status.idle":"2021-09-17T15:07:29.789960Z","shell.execute_reply.started":"2021-09-17T15:07:29.754651Z","shell.execute_reply":"2021-09-17T15:07:29.789236Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(torch.Size([2, 512]), torch.Size([2, 512]), torch.Size([2, 512]))"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-4)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:29.791331Z","iopub.execute_input":"2021-09-17T15:07:29.791589Z","iopub.status.idle":"2021-09-17T15:07:35.295840Z","shell.execute_reply.started":"2021-09-17T15:07:29.791556Z","shell.execute_reply":"2021-09-17T15:07:35.295168Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def train(data_loader, model, tokenizer, optimizer, device):\n    model.train()\n\n    total_steps = len(data_loader)\n    epoch_loss = 0\n\n    for idx, batch in enumerate(data_loader):\n        optimizer.zero_grad()\n\n        ids = batch[\"source_ids\"].to(device)\n        mask = batch[\"source_mask\"].to(device)\n\n        target_ids = batch[\"target_ids\"].to(device)\n        \n        y_ids = target_ids[:, :-1].contiguous()\n        lm_labels = target_ids[:, 1:].clone().detach()\n        lm_labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n\n        outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            decoder_input_ids=y_ids,\n            labels=lm_labels\n        )\n\n        loss = outputs[0]\n        epoch_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n\n        if idx%100 == 0:\n            print(f\"Step: {idx}/{total_steps} | Loss: {loss.item()}\")\n    \n    return epoch_loss / total_steps","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:35.299396Z","iopub.execute_input":"2021-09-17T15:07:35.299865Z","iopub.status.idle":"2021-09-17T15:07:35.315890Z","shell.execute_reply.started":"2021-09-17T15:07:35.299836Z","shell.execute_reply":"2021-09-17T15:07:35.315232Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:35.319734Z","iopub.execute_input":"2021-09-17T15:07:35.322174Z","iopub.status.idle":"2021-09-17T15:07:35.913506Z","shell.execute_reply.started":"2021-09-17T15:07:35.322137Z","shell.execute_reply":"2021-09-17T15:07:35.912444Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"best_valid_loss = float('inf')\n\nfor epoch in range(1):\n    start_time = time.time()\n    train_loss = train(train_data_loader, model, tokenizer, optimizer, device)\n    #val_loss = evaluate(val_data_loader, model, TOKENIZER, device)\n    end_time = time.time()\n   \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    #if val_loss < best_valid_loss:\n    #    best_valid_loss = val_loss\n    #    torch.save(model.state_dict(), MODEL_PATH)\n    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n    print(f\"\\t Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):5.4f}\")\n    #print(f\"\\t Val Loss: {val_loss:.3f} | Val PPL: {np.exp(val_loss):5.4f}\")\nwriter.flush()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:07:35.916118Z","iopub.execute_input":"2021-09-17T15:07:35.916538Z","iopub.status.idle":"2021-09-17T15:32:13.089600Z","shell.execute_reply.started":"2021-09-17T15:07:35.916497Z","shell.execute_reply":"2021-09-17T15:32:13.088145Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Step: 0/10000 | Loss: 2.282130479812622\nStep: 100/10000 | Loss: 0.8982489109039307\nStep: 200/10000 | Loss: 4.225487232208252\nStep: 300/10000 | Loss: 1.317039132118225\nStep: 400/10000 | Loss: 2.7486627101898193\nStep: 500/10000 | Loss: 2.058995485305786\nStep: 600/10000 | Loss: 2.9621689319610596\nStep: 700/10000 | Loss: 1.0606534481048584\nStep: 800/10000 | Loss: 1.741492509841919\nStep: 900/10000 | Loss: 0.9447327256202698\nStep: 1000/10000 | Loss: 2.990053176879883\nStep: 1100/10000 | Loss: 1.050997018814087\nStep: 1200/10000 | Loss: 1.4592121839523315\nStep: 1300/10000 | Loss: 2.662625789642334\nStep: 1400/10000 | Loss: 1.0493813753128052\nStep: 1500/10000 | Loss: 2.137808322906494\nStep: 1600/10000 | Loss: 1.5480986833572388\nStep: 1700/10000 | Loss: 2.8785765171051025\nStep: 1800/10000 | Loss: 2.0084145069122314\nStep: 1900/10000 | Loss: 2.365753650665283\nStep: 2000/10000 | Loss: 1.8568999767303467\nStep: 2100/10000 | Loss: 1.7412028312683105\nStep: 2200/10000 | Loss: 2.6733286380767822\nStep: 2300/10000 | Loss: 1.6691648960113525\nStep: 2400/10000 | Loss: 1.7853716611862183\nStep: 2500/10000 | Loss: 1.6265428066253662\nStep: 2600/10000 | Loss: 1.8779962062835693\nStep: 2700/10000 | Loss: 2.5325677394866943\nStep: 2800/10000 | Loss: 1.936384916305542\nStep: 2900/10000 | Loss: 1.4623241424560547\nStep: 3000/10000 | Loss: 1.780601143836975\nStep: 3100/10000 | Loss: 1.6587671041488647\nStep: 3200/10000 | Loss: 2.0997703075408936\nStep: 3300/10000 | Loss: 1.1788326501846313\nStep: 3400/10000 | Loss: 1.0686705112457275\nStep: 3500/10000 | Loss: 1.469934344291687\nStep: 3600/10000 | Loss: 1.555597186088562\nStep: 3700/10000 | Loss: 1.8858319520950317\nStep: 3800/10000 | Loss: 1.5424447059631348\nStep: 3900/10000 | Loss: 2.7446718215942383\nStep: 4000/10000 | Loss: 1.962158441543579\nStep: 4100/10000 | Loss: 2.316732406616211\nStep: 4200/10000 | Loss: 0.972459614276886\nStep: 4300/10000 | Loss: 1.9849436283111572\nStep: 4400/10000 | Loss: 0.7219275832176208\nStep: 4500/10000 | Loss: 1.8934919834136963\nStep: 4600/10000 | Loss: 1.861959457397461\nStep: 4700/10000 | Loss: 2.0815722942352295\nStep: 4800/10000 | Loss: 1.1608136892318726\nStep: 4900/10000 | Loss: 3.2497899532318115\nStep: 5000/10000 | Loss: 2.1991147994995117\nStep: 5100/10000 | Loss: 1.585789680480957\nStep: 5200/10000 | Loss: 2.0264909267425537\nStep: 5300/10000 | Loss: 1.7413578033447266\nStep: 5400/10000 | Loss: 0.9066892862319946\nStep: 5500/10000 | Loss: 1.6720470190048218\nStep: 5600/10000 | Loss: 3.246095895767212\nStep: 5700/10000 | Loss: 1.5712391138076782\nStep: 5800/10000 | Loss: 1.9397270679473877\nStep: 5900/10000 | Loss: 1.0700527429580688\nStep: 6000/10000 | Loss: 2.991867780685425\nStep: 6100/10000 | Loss: 0.9358215928077698\nStep: 6200/10000 | Loss: 1.0649513006210327\nStep: 6300/10000 | Loss: 1.5468508005142212\nStep: 6400/10000 | Loss: 0.92710942029953\nStep: 6500/10000 | Loss: 3.319220781326294\nStep: 6600/10000 | Loss: 1.6885864734649658\nStep: 6700/10000 | Loss: 2.1209206581115723\nStep: 6800/10000 | Loss: 2.2931058406829834\nStep: 6900/10000 | Loss: 0.3305661678314209\nStep: 7000/10000 | Loss: 0.8169518113136292\nStep: 7100/10000 | Loss: 2.8104805946350098\nStep: 7200/10000 | Loss: 1.506049633026123\nStep: 7300/10000 | Loss: 1.416059970855713\nStep: 7400/10000 | Loss: 1.3191313743591309\nStep: 7500/10000 | Loss: 2.67868971824646\nStep: 7600/10000 | Loss: 1.113635778427124\nStep: 7700/10000 | Loss: 2.5998878479003906\nStep: 7800/10000 | Loss: 1.6837266683578491\nStep: 7900/10000 | Loss: 1.2051417827606201\nStep: 8000/10000 | Loss: 2.733767509460449\nStep: 8100/10000 | Loss: 1.141687273979187\nStep: 8200/10000 | Loss: 1.790265440940857\nStep: 8300/10000 | Loss: 1.8486357927322388\nStep: 8400/10000 | Loss: 2.220334053039551\nStep: 8500/10000 | Loss: 2.0313150882720947\nStep: 8600/10000 | Loss: 2.581758975982666\nStep: 8700/10000 | Loss: 0.6853472590446472\nStep: 8800/10000 | Loss: 0.6868476271629333\nStep: 8900/10000 | Loss: 2.4214913845062256\nStep: 9000/10000 | Loss: 1.638602614402771\nStep: 9100/10000 | Loss: 1.7803740501403809\nStep: 9200/10000 | Loss: 1.0020135641098022\nStep: 9300/10000 | Loss: 1.5665732622146606\nStep: 9400/10000 | Loss: 1.9427515268325806\nStep: 9500/10000 | Loss: 0.7231573462486267\nStep: 9600/10000 | Loss: 2.9095561504364014\nStep: 9700/10000 | Loss: 1.4152828454971313\nStep: 9800/10000 | Loss: 2.4165151119232178\nStep: 9900/10000 | Loss: 0.668579638004303\nEpoch: 01 | Epoch Time: 24m 37s\n\t Train Loss: 1.845 | Train PPL: 6.3295\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, './model')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:39:06.999186Z","iopub.execute_input":"2021-09-17T15:39:06.999590Z","iopub.status.idle":"2021-09-17T15:39:07.681078Z","shell.execute_reply.started":"2021-09-17T15:39:06.999552Z","shell.execute_reply":"2021-09-17T15:39:07.680269Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def inference(model, phrase, tokenizer, device):\n    model.eval()\n\n    paraphrase = \"paraphrase: \" + phrase\n\n    source = tokenizer.encode_plus(\n        phrase,\n        max_length=512,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        truncation=True,\n        return_tensors='pt')\n    \n    with torch.no_grad():\n        ids = source[\"input_ids\"].to(device)\n        mask = source[\"attention_mask\"].to(device)\n\n        generated_ids = model.generate(\n            input_ids=ids,\n            attention_mask=mask,\n            max_length=512,\n            num_beams=2,\n            repetition_penalty=2.5,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n\n        summary = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n\n    return summary[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:35:38.639527Z","iopub.execute_input":"2021-09-17T15:35:38.640463Z","iopub.status.idle":"2021-09-17T15:35:38.649681Z","shell.execute_reply.started":"2021-09-17T15:35:38.640413Z","shell.execute_reply":"2021-09-17T15:35:38.648615Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"phrase = \"I should have positive attitude\"\nsummary = inference(model, phrase, tokenizer, device)\nsummary","metadata":{"execution":{"iopub.status.busy":"2021-09-17T15:37:14.964863Z","iopub.execute_input":"2021-09-17T15:37:14.965590Z","iopub.status.idle":"2021-09-17T15:37:15.136684Z","shell.execute_reply.started":"2021-09-17T15:37:14.965546Z","shell.execute_reply":"2021-09-17T15:37:15.135824Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'I should have a positive attitude towards my life.'"},"metadata":{}}]}]}