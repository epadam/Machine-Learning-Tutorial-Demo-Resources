{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUAD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4Nhmhl58Ep9ZXjja5SJuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epadam/Machine-Learning-Tutorial-Demo-Resources/blob/master/notebooks/nlp/CUAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3P4NLJzj3tD",
        "outputId": "d9b55f35-9b0c-4741-b372-af8334aa2acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuad'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 30 (delta 10), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/TheAtticusProject/cuad.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv cuad cuad-training"
      ],
      "metadata": {
        "id": "cBxtpt5kl2xM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cuad-training/data.zip -d cuad-data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2QspZq8IDPu",
        "outputId": "371d0f2c-129f-4fbb-e3f7-13d2601569d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cuad-training/data.zip\n",
            "  inflating: cuad-data/CUADv1.json   \n",
            "  inflating: cuad-data/test.json     \n",
            "  inflating: cuad-data/train_separate_questions.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir cuad-models"
      ],
      "metadata": {
        "id": "2TNHWeMpIHpb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://zenodo.org/record/4599830/files/roberta-base.zip?download=1 --output cuad-models/roberta-base.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-nqy2P-ILYn",
        "outputId": "c3cfe79e-ca0e-4418-bfe3-1eb72128d437"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  426M  100  426M    0     0  19.9M      0  0:00:21  0:00:21 --:--:-- 18.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cuad-models/roberta-base.zip -d cuad-models/"
      ],
      "metadata": {
        "id": "r_OADSu9IPbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "lFZGkqJpIS7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    squad_convert_examples_to_features\n",
        ")\n"
      ],
      "metadata": {
        "id": "i71wlIVfXbjH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits"
      ],
      "metadata": {
        "id": "InYRT-YtbhMt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_prediction(question_texts, context_text, model_path):\n",
        "    ### Setting hyperparameters\n",
        "    max_seq_length = 512\n",
        "    doc_stride = 256\n",
        "    n_best_size = 1\n",
        "    max_query_length = 64\n",
        "    max_answer_length = 512\n",
        "    do_lower_case = False\n",
        "    null_score_diff_threshold = 0.0\n",
        "\n",
        "    # model_name_or_path = \"../cuad-models/roberta-base/\"\n",
        "\n",
        "    def to_list(tensor):\n",
        "        return tensor.detach().cpu().tolist()\n",
        "\n",
        "    config_class, model_class, tokenizer_class = (\n",
        "        AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer)\n",
        "    config = config_class.from_pretrained(model_path)\n",
        "    tokenizer = tokenizer_class.from_pretrained(\n",
        "        model_path, do_lower_case=True, use_fast=False)\n",
        "    model = model_class.from_pretrained(model_path, config=config)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    processor = SquadV2Processor()\n",
        "    examples = []\n",
        "\n",
        "    for i, question_text in enumerate(question_texts):\n",
        "        example = SquadExample(\n",
        "            qas_id=str(i),\n",
        "            question_text=question_text,\n",
        "            context_text=context_text,\n",
        "            answer_text=None,\n",
        "            start_position_character=None,\n",
        "            title=\"Predict\",\n",
        "            answers=None,\n",
        "        )\n",
        "\n",
        "        examples.append(example)\n",
        "\n",
        "    features, dataset = squad_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=max_seq_length,\n",
        "        doc_stride=doc_stride,\n",
        "        max_query_length=max_query_length,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        threads=1,\n",
        "    )\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            for i, example_index in enumerate(example_indices):\n",
        "                eval_feature = features[example_index.item()]\n",
        "                unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "                output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
        "\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "                all_results.append(result)\n",
        "\n",
        "    final_predictions = compute_predictions_logits(\n",
        "        all_examples=examples,\n",
        "        all_features=features,\n",
        "        all_results=all_results,\n",
        "        n_best_size=n_best_size,\n",
        "        max_answer_length=max_answer_length,\n",
        "        do_lower_case=do_lower_case,\n",
        "        output_prediction_file=None,\n",
        "        output_nbest_file=None,\n",
        "        output_null_log_odds_file=None,\n",
        "        verbose_logging=False,\n",
        "        version_2_with_negative=True,\n",
        "        null_score_diff_threshold=null_score_diff_threshold,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    return final_predictions"
      ],
      "metadata": {
        "id": "HcIIAZdMgjfT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "AYkdEQ_fqa_A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./cuad-data/CUADv1.json') as json_file:\n",
        "    data = json.load(json_file)"
      ],
      "metadata": {
        "id": "QZnBDe5FqjRG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "for i, q in enumerate(data['data'][0]['paragraphs'][0]['qas']):\n",
        "    question = data['data'][0]['paragraphs'][0]['qas'][i]['question']\n",
        "    questions.append(question)\n",
        "contract = data['data'][0]['paragraphs'][0]['context']"
      ],
      "metadata": {
        "id": "mBdzvZU-qmNe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('contract.txt', 'a') as f:\n",
        "    f.write(' '.join(contract.split()))"
      ],
      "metadata": {
        "id": "E6sytF2Vre07"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = run_prediction(questions, contract, 'cuad-models/roberta-base/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhwY9wXmrrwg",
        "outputId": "13700fe2-ad0e-496e-d6b2-a58648bad0ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "convert squad examples to features: 100%|██████████| 41/41 [00:42<00:00,  1.03s/it]\n",
            "add example index and unique id: 100%|██████████| 41/41 [00:00<00:00, 42127.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('predictions.txt', 'a') as f:\n",
        "    for i, p in enumerate(predictions):\n",
        "        f.write(f\"Question {i+1}: {questions[int(p)]}\\nAnswer: {predictions[p]}\\n\\n\")"
      ],
      "metadata": {
        "id": "-x3nqVIvuYma"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}