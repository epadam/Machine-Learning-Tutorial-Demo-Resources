# Machine Learning

## Traditional Algorithms

### Regression

### Decision Tree

### Random Forest

### Gradient Boosting

### K-means

### Bayesian

### Enssemble Algorithms

### Support Vector Machine (SVM)

## Deep Learning

### CNN

### Feature Pyramid Networks

### Fully Convolutional Networks

### Capsule Neural Network

* [Capsule Neural Network](https://github.com/Sarasra/models/tree/master/research/capsules)

### Autoencoder

### Variational Autoencoder

### Generative Adversarial Network (GAN)

### VAE-GAN

### Graph Networks

* Relational inductive biases, deep learning, and graph networks [`arXiv`](https://arxiv.org/abs/1806.01261)

* Graph Convolutional Networks

### RNN

### LSTM

### Temporal Convolutional Network

### Attention

### Transformer



## ML Training

### Loss Function

### Optimization

### Regularization

### Batch Normalization

### Tools

[gradient-checkpointing](https://github.com/cybertronai/gradient-checkpointing)



## Supervised Learning

## Semi-Supervised Learning (mix labeled and unlabeled data)

[MixMatch](https://github.com/google-research/mixmatch)

## Weakly Supervised Learning


## Self-Supervised Learning (Unsupervised Learning)

Self-supervised learning means training the models without labeled data. AutoEncoder, GAN are examples in CV, while pre-trained model like Bert is example in NLP.

[UGATIT](https://github.com/taki0112/UGATIT?utm_source=mybridge&utm_medium=blog&utm_campaign=read_more)

## How to add new class to a trained model

* Life Long Learning

## How to improve the model when more data is available

* Incremental Training

## Reference
* [Awesome Self-Supervised Learning](https://github.com/jason718/awesome-self-supervised-learning)



